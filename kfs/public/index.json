[
{
	"uri": "http://example.org/kubernetes/",
	"title": "Kubernetes",
	"tags": ["Kubernetes"],
	"description": "",
	"content": "关于 Kubernetes 本身的一些文章。\n"
},
{
	"uri": "http://example.org/kubernetes/components/",
	"title": "组件",
	"tags": ["Kubernetes", "Components"],
	"description": "",
	"content": " 参考  Kubernetes Components A Complete Introduction to Kubernetes — an Orchestration Tool for Containers  Components Kubernetes Master (控制节点) Master 包含一组应用程序（组件），通常部署在一台服务器上，也可以分别部署在多台服务器。\netcd Kubernetes 集群数据全部存储在 etcd （通常是集群）中。\nkube-apiserver Kubernetes 集群对外 API 接口服务。\nkube-scheduler 依据调度策略，为 Pod 分配可用的 Node 节点。\nkube-controller-manager 运行一系列的 controllers 。\ncloud-controller-manager 运行在一些云平台中，与之集成。\n进阶 如何创建高可用的 Kubernetes 集群？\n Creating Highly Available Clusters with kubeadm  Kubernetes Node (工作节点) Kubernetes Node 通常运行以下组件。\nkubelet 集群中每一个 Node 都需要运行 kubelet 。kubelet 运行并管理有 Kubernetes 创建的 Pod 。\nkube-proxy Kubernetes 对于服务 (Service)的抽象网络，需要 kube-proxy 在 Node 上执行特定的规则才能生效。同时 kube-proxy 也提供转发服务。\nContainer Runtime Kubernetes 支持： - Docker - containerd - cri-o - rktlet - 任何支持 Kubernetes CRI (Container Runtime Interface) 标准的容器 Runtime\n"
},
{
	"uri": "http://example.org/kfs/",
	"title": "KFS",
	"tags": ["kubernetes", "istio"],
	"description": "",
	"content": "KFS 是 Kubernetes From Scratch 的缩写，项目命名学习 LFS (Linux From Sscrath)，同时在实践风格方面也学习 LFS。\n"
},
{
	"uri": "http://example.org/kubernetes/node/",
	"title": "节点",
	"tags": ["Kubernetes", "node"],
	"description": "",
	"content": "参考 Nodes\nKubernetes 中的节点 （ node ） 即工作节点 ( worker machine ）\n"
},
{
	"uri": "http://example.org/kubectl/",
	"title": "kubectl",
	"tags": ["Kubernetes", "kubectl"],
	"description": "",
	"content": " kubectl 是 Kubernetes 的一个命令行工具，可以执行各种操作。目标是成为 git 一样的使用方法（风格），和一样的领域地位的工具。\n 官方文档  brew link --overwrite kubernetes-cli # 覆盖 docker 安装的版本 brew link --overwrite kubernetes-cli # 检查版本 kubectl version Tips 设置默认的配置文件 export KUBECONFIG=\u0026#34;/path/to/my.kubeconfig\u0026#34; kubectl get all --all-namespaces"
},
{
	"uri": "http://example.org/kfs/v1.14.1/lab-design/",
	"title": "实验环境规划",
	"tags": ["v1.14.1", "design"],
	"description": "",
	"content": " hk-1 位于阿里云香港区的一台 VPS 。\n配置\n 1核 CPU 512M 内存 按流量计费带宽  用途\n Google 下载软件包，如 kubernetes 传递一些无法访问的 image ，参考 DOCKER IMAGE 在QIANG外怎么办？  mbp 实验用的 Macbook Pro 。\n配置\n 16G 内存  说明\n 如无特殊说明， kubectl 命令的执行都在 mbp （通过配置 $KUBECONFIG 环境变量指向 admin-mbp.kubeconfig 路径实现访问集群的许可）  虚拟机 在 mbp 上，通过 vagrant + virtualbox 运行实验需要的 3 台虚拟机。\n   主机名 IP 配置 角色 说明     k8s-master-1 192.168.100.11 2核CPU，2048M 内存 Master 部署 Kubernetes Master 组件   k8s-node-1 192.168.100.31 2核CPU，2048M 内存 Node 部署 Kubernetes Node 组件   k8s-node-2 192.168.100.32 2核CPU，2048M 内存 Node 同上    Master 角色虚拟机需要安装：\n etcd kube-apiserver kube-controller-manager kube-scheduler flanneld  Node 角色虚拟机需要安装：\n kubelet kube-proxy containerd flanneld  "
},
{
	"uri": "http://example.org/kfs/v1.14.1/env/",
	"title": "设置环境变量",
	"tags": ["v1.14.1", "env"],
	"description": "",
	"content": "在 mbp 创建实验工作目录：\nexport KFS_HOME=~/kfslab mkdir -pv $KFS_HOME cd $KFS_HOME 实验过程中，为了保持环境设置一致，我们创建一个 setting 文件，存放用到的环境变量，每次离开后再次返回，需要重新使 setting 中的环境变量生效。\n创建 setting 文件：\ncd $KFS_HOME cat \u0026gt; setting \u0026lt;\u0026lt;EOF export KFS_K8S_VERSION=v1.14.1 export KFS_HOME=~/kfslab export KFS_K8S_PKG_DIR=\u0026#34;\\${KFS_HOME}/\\${KFS_K8S_VERSION}\u0026#34; export KFS_VOS=\u0026#34;\\${KFS_HOME}/vos\u0026#34; export KFS_CONFIG=\u0026#34;\\${KFS_HOME}/config\u0026#34; export KFS_INSTALL=\u0026#34;\\${KFS_HOME}/install\u0026#34; export KUBERNETES_PUBLIC_ADDRESS=\u0026#34;192.168.100.11\u0026#34; EOF source setting 每次中断实验后，再次返回，先使得 setting 文件中的环境变量生效：\ncd $KFS_HOME source setting 说明\n 默认我们使用 ~/kfslab 作为 KFS 实验的主目录，后面的实验步骤中我们会使用 $KFS_HOME 环境变量代替该值。 创建一个文件，可以使用任何喜欢的编辑器，注意文件所在的目录即可。示例，创建名为 setting 的文件，可以在终端当前目录执行 vim setting ，编辑该文件内容，保存退出即可。  "
},
{
	"uri": "http://example.org/istio/",
	"title": "Istio",
	"tags": ["kubernetes", "istio"],
	"description": "",
	"content": "  https://istio.io  安装 参考 [Customizable Install with Helm](https://istio.io/docs/setup/kubernetes/install/helm\ncurl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.1.4 sh -cd istio-1.1.4 cp bin/istioctl /usr/local/bin 已经初始化并按照 tiller 的步骤：\nkubectl apply -f install/kubernetes/helm/helm-service-account.yaml helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system # 检查 crds 是否为 53 kubectl get crds | grep \u0026#39;istio.io\\|certmanager.k8s.io\u0026#39; | wc -l # 部署默认配置 helm install install/kubernetes/helm/istio --name istio --namespace istio-system"
},
{
	"uri": "http://example.org/kfs/v1.14.1/prepare/",
	"title": "准备工作",
	"tags": ["v1.14.1", "Kubernetes", "prepare"],
	"description": "",
	"content": " 准备 SSH 在 mbp 配置 ~/.ssh/config , 确保相关服务器可以公钥免密登录，如：登录 hk-1\nssh hk-1 下载 Kubernetes 登录 hk-1 服务器，下载 Kuberentes：\n 从 https://github.com/kubernetes/kubernetes/releases 选择一个合适的版本 下载指定的版本，本次实验使用 v1.14.1  # 创建一个和版本名称一样的子目录 mkdir -pv ~/k8s/v1.14.1 cd ~/k8s/v1.14.1/ # 下载 wget https://github.com/kubernetes/kubernetes/releases/download/v1.14.1/kubernetes.tar.gz # 解压 tar xf kubernetes.tar.gz # 下载 Kubernetes 二进制文件 cd kubernetes/cluster/ ./get-kube-binaries.sh # 上面命令提示下载位置，输入 y 以示确认 下载完成后，拷贝 hk-1 服务器上的 v1.14.1 目录到 mbp 的 $KFS_HOME ，在 mbp 执行：\n# hk-1 是我在香港的服务器 cd $KFS_HOME mkdir -pv $KFS_K8S_PKG_DIR rsync -avz --progress hk-1:~/k8s/v1.14.1 $KFS_K8S_PKG_DIR # 解压二进制程序 cd $KFS_K8S_PKG_DIR/kubernetes/server tar xf kubernetes-server-linux-amd64.tar.gz 同步完成后，拷贝 master , node 需要的二进制程序：\n# 创建目录 mkdir -pv ${KFS_INSTALL}/{master,node}/bin cd $KFS_K8S_PKG_DIR/kubernetes/server/kubernetes/server/bin/ # 拷贝 master 需要的二进制程序 cp kubectl kube-apiserver kube-scheduler kube-controller-manager ${KFS_INSTALL}/master/bin/ # 拷贝 node 需要的二进制程序 cp kubectl kubelet kube-proxy ${KFS_INSTALL}/node/bin etcd 从 https://github.com/etcd-io/etcd/releases 下载合适的版本，以 v3.3.12 为例：\nmkdir -pv ${KFS_INSTALL}/master/bin ETCD_VER=v3.3.12 # choose either URL GOOGLE_URL=https://storage.googleapis.com/etcd GITHUB_URL=https://github.com/etcd-io/etcd/releases/download DOWNLOAD_URL=${GITHUB_URL} rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz rm -rf /tmp/etcd-download-test \u0026amp;\u0026amp; mkdir -p /tmp/etcd-download-test curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1 rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz # 复制到 master/bin cp /tmp/etcd-download-test/etcd* ${KFS_INSTALL}/master/bin/ cfssl 在 mbp 执行：\nbrew install cfssl kubectl MacOS 上如果安装了 Docker , 默认已经有 kubectl 命令（但是通常版本比较旧）：\nls -al /usr/local/bin/kubectl lrwxr-xr-x 1 gwind staff 55 Jan 25 20:45 /usr/local/bin/kubectl -\u0026gt; /Applications/Docker.app/Contents/Resources/bin/kubectl 我们可以重新安装最新的 kubectl ：\nbrew install kubernetes-cli # 覆盖 docker 安装的版本 brew link --overwrite kubernetes-cli # 检查版本 kubectl version vagrant \u0026amp; virtualbox \u0026amp; ansible brew cask install virtualbox brew cask install vagrant brew cask install vagrant-manager brew install ansible"
},
{
	"uri": "http://example.org/kfs/v1.14.1/vagrant/",
	"title": "搭建实验环境",
	"tags": ["v1.14.1", "vagrant"],
	"description": "",
	"content": " 说明\n 实践过程中，我们需要确保 mbp 的 $KFS_HOME 和其他 3 个虚拟机(k8s-master-1, k8s-node-1, k8s-node-2) 的 /kfslab 目录保持一致。 如果使用 virtualbox 挂载 Host (mbp) $KFS_HOME 目录到每一个虚拟机的 /kfslab 目录，请参考 安装 vagrant-vbguest 插件 。否则请使用 rsync 等工具，及时同步 mbp 的 $KFS_HOME 到每一个虚拟机 的 /kfslab 目录。  创建 vagrant 配置 在 mbp 执行\n1. 创建 Vagrantfile 说明\n vagrant 主目录（含 Vagrantfile 的目录）不要存放非必要文件，比如 kubernetes 下载包等。 因为 vagrant 默认会挂载当前目录到虚拟机的 /vagrant 。Synced Folders\n cd $KFS_VOS vim Vagrantfile 创建 Vagrantfile 内容如下：\nIMAGE_NAME = \u0026#34;centos/7\u0026#34; TOTAL_MASTER = 1 TOTAL_NODE = 2 if Vagrant.has_plugin?(\u0026#34;vagrant-vbguest\u0026#34;) config.vbguest.auto_update = false end Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.ssh.insert_key = false config.vm.box = IMAGE_NAME config.vm.box_check_update = false # 禁止默认的目录同步行为 config.vm.synced_folder \u0026#34;.\u0026#34;, \u0026#34;/vagrant\u0026#34;, disabled: true # 共享目录（需要安装好 guest 驱动才可以注销掉下面几行） # config.vm.synced_folder ENV[\u0026#34;KFS_HOME\u0026#34;], \u0026#34;/kfslab\u0026#34;, type: \u0026#34;virtualbox\u0026#34; # if Vagrant.has_plugin?(\u0026#34;vagrant-vbguest\u0026#34;) # config.vbguest.auto_update = false  # end config.vm.provider \u0026#34;virtualbox\u0026#34; do |v| v.memory = 2048 v.cpus = 2 end (1..TOTAL_MASTER).each do |i| config.vm.define \u0026#34;k8s-master-#{i}\u0026#34; do |master| master.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.100.#{i + 10}\u0026#34; master.vm.hostname = \u0026#34;k8s-master-#{i}\u0026#34; master.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.playbook = \u0026#34;kubernetes-setup/master-playbook.yml\u0026#34; end end end (1..TOTAL_NODE).each do |i| config.vm.define \u0026#34;k8s-node-#{i}\u0026#34; do |node| node.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.100.#{i + 30}\u0026#34; node.vm.hostname = \u0026#34;k8s-node-#{i}\u0026#34; node.vm.provision \u0026#34;ansible\u0026#34; do |ansible| ansible.playbook = \u0026#34;kubernetes-setup/node-playbook.yml\u0026#34; end end end end 2. 创建 kubernetes-setup 目录 mkdir -pv kubernetes-setup 3. 创建 kubernetes-setup/master-playbook.yml vim kubernetes-setup/master-playbook.yml 创建 kubernetes-setup/master-playbook.yml 内容如下：\n- hosts: all become: true roles: - geerlingguy.repo-epel tasks: # Disable SELinux - selinux: state: disabled - name: Disable SWAP since kubernetes can\u0026#39;t work with swap enabled (1/2) shell: | swapoff -a - name: Disable SWAP in fstab since kubernetes can\u0026#39;t work with swap enabled (2/2) replace: path: /etc/fstab regexp: \u0026#39;^([^#].+?\\sswap\\s+.*)$\u0026#39; replace: \u0026#39;# \\1\u0026#39; - name: update centos yum: name: \u0026#39;*\u0026#39; state: latest - name: install basic packages yum: name: \u0026#34;{{ packages }}\u0026#34; vars: packages: - wget - vim - htop - dstat - lsof - tree - tmux - rsync - ntp - nginx 4. 创建 kubernetes-setup/node-playbook.yml vim kubernetes-setup/node-playbook.yml 创建 kubernetes-setup/node-playbook.yml 内容如下：\n- hosts: all become: true roles: - geerlingguy.repo-epel tasks: - name: 禁用 selinux selinux: state: disabled - name: 停用 swap 分区 (kubelet) (1/2) shell: | swapoff -a - name: 修改 fstab ，系统下次启动时禁用 swap 分区 (2/2) replace: path: /etc/fstab regexp: \u0026#39;^([^#].+?\\sswap\\s+.*)$\u0026#39; replace: \u0026#39;# \\1\u0026#39; - name: update centos yum: name: \u0026#39;*\u0026#39; state: latest - name: install basic packages yum: name: \u0026#34;{{ packages }}\u0026#34; vars: packages: - wget - vim - htop - dstat - lsof - tree - tmux - rsync - ntp - socat - conntrack - ipset 创建虚拟机并启动 vagrant up 登录虚拟机  实验中执行命令的用户，如果无特殊说明，默认是 root 用户\n 以 k8s-master-1 虚拟机为例，执行：\n$ cd $KFS_HOME $ vagrant ssh k8s-master-1 [vagrant@k8s-master-1 ~]$ sudo -i [root@k8s-master-1 ~]# mkdir -p .ssh [root@k8s-master-1 ~]# vim .ssh/authorized_keys # 粘贴 mbp 上的 ~/.ssh/id_rsa.pub 内容，保存即可 在 mbp 的 ~/.ssh/config 添加配置：\nHost k8s-master-1 port 22 user root hostname 192.168.100.11 现在可以从 mbp 通过 ssh 登录 k8s-master-1 服务器：\nssh -v k8s-master-1 其他虚拟机 ( k8s-node-1, k8s-node-2 ) 操作类似。\n检查 操作系统 确认 k8s-master-1 , k8s-node-1 , k8s-node-2 操作系统配置：\n 禁止 selinux swapoff -a , 并修改 /etc/fstab 禁止 swap 分区  FAQ 虚拟机 的 eth0 IP 全部是 10.0.2.15  eth0 as NAT is a fundamental requirement of Vagrant in its current state  Read-only file system (RuntimeError) 参考 trun off swap cause vagrant up failed\nvagrant up 详细错误如下:\n==\u0026gt; k8s-node-1: Configuring and enabling network interfaces... /opt/vagrant/embedded/gems/2.2.4/gems/net-scp-1.2.1/lib/net/scp.rb:398:in `await_response_state\u0026#39;: scp: /tmp/vagrant-network-entry-eth1-1557190901-0: Read-only file system (RuntimeError) from /opt/vagrant/embedded/gems/2.2.4/gems/net-scp-1.2.1/lib/net/scp.rb:369:in `block (3 levels) in start_command\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/net-ssh-5.1.0/lib/net/ssh/connection/channel.rb:323:in `process\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/net-ssh-5.1.0/lib/net/ssh/connection/session.rb:250:in `block in ev_preprocess\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/net-ssh-5.1.0/lib/net/ssh/connection/session.rb:540:in `each\u0026#39; 这里不是参考链接中提到的 swap 问题，而是我们修改 swap 分区时错误地把根分区注释掉，因此出错。\n虽然报错，但是服务器已经启动。登录服务器，检查 /etc/fstab 最后两行，发现：\n# cat /etc/fstab # # /etc/fstab # Created by anaconda on Thu Feb 28 20:50:01 2019 # # Accessible filesystems, by reference, are maintained under \u0026#39;/dev/disk\u0026#39; # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # # UUID=f52f361a-da1a-4ea0-8c7f-ca2706e86b46 / xfs defaults 0 0 # /swapfile none swap defaults 0 0 查看根分区确实挂载为 ro :\n# mount |grep xfs /dev/sda1 on / type xfs (ro,relatime,attr2,inode64,noquota) 重新挂载根分区：\nmount -o remount,rw --uuid f52f361a-da1a-4ea0-8c7f-ca2706e86b46 / symlink has no referent vagrant up 详细错误如下：\n==\u0026gt; k8s-master-1: Rsyncing folder: /Users/gwind/kfslab/ =\u0026gt; /vagrant There was an error when attempting to rsync a synced folder. Please inspect the error message below for more info. Host path: /Users/gwind/kfslab/ Guest path: /vagrant Command: \u0026#34;rsync\u0026#34; \u0026#34;--verbose\u0026#34; \u0026#34;--archive\u0026#34; \u0026#34;--delete\u0026#34; \u0026#34;-z\u0026#34; \u0026#34;--copy-links\u0026#34; \u0026#34;--no-owner\u0026#34; \u0026#34;--no-group\u0026#34; \u0026#34;--rsync-path\u0026#34; \u0026#34;sudo rsync\u0026#34; \u0026#34;-e\u0026#34; \u0026#34;ssh -p 2200 -o LogLevel=FATAL -o ControlMaster=auto -o ControlPath=/var/folders/5s/fcy1xt5s51d7r6xljhmxh qj80000gn/T/vagrant-rsync-20190507-75080-1povtee -o ControlPersist=10m -o IdentitiesOnly=yes -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i \u0026#39;/Users/gwind/.vagrant.d/insecure_private_key\u0026#39;\u0026#34; \u0026#34;--exclude\u0026#34; \u0026#34;.vagrant/\u0026#34; \u0026#34;/Users/g wind/kfslab/\u0026#34; \u0026#34;vagrant@127.0.0.1:/vagrant\u0026#34; Error: /etc/profile.d/lang.sh: line 19: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory symlink has no referent: \u0026#34;/Users/gwind/kfslab/v1.14.1/kubernetes/client/bin\u0026#34; rsync error: some files could not be transferred (code 23) at /BuildRoot/Library/Caches/com.apple.xbs/Sources/rsync/rsync-52.200.1/rsync/main.c(996) [sender=2.6.9] 原因是 kubernetes 在 Vagrantfile 同一级目录。\n禁用 synced folders 参考 Disabling synced folders\n默认情况下，vagrant 会将 Vagrantfile 所在的目录同步到虚拟机的 /vagrant 目录。可以禁用这一行为。\nsynced folders 不能实时同步 检查 synced_folders 实现：\n➜ vos cat .vagrant/machines/k8s-master-1/virtualbox/synced_folders {\u0026#34;rsync\u0026#34;:{\u0026#34;/vagrant\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;rsync\u0026#34;,\u0026#34;guestpath\u0026#34;:\u0026#34;/vagrant\u0026#34;,\u0026#34;hostpath\u0026#34;:\u0026#34;/Users/gwind/kfslab/vos\u0026#34;,\u0026#34;disabled\u0026#34;:false,\u0026#34;__vagrantfile\u0026#34;:true,\u0026#34;owner\u0026#34;:\u0026#34;vagrant\u0026#34;,\u0026#34;group\u0026#34;:\u0026#34;vagrant\u0026#34;}}} 发现使用了 rsync\n配置使用 virtualbox 即可。\n参考 Updated CentOS Vagrant Images Available (v1704.01)\nVagrant was unable to mount VirtualBox shared folders 提示 guest OS (CentOS 7) 缺少 vboxsf ：\n==\u0026gt; k8s-master-1: Mounting shared folders... k8s-master-1: /kfslab =\u0026gt; /Users/gwind/kfslab Vagrant was unable to mount VirtualBox shared folders. This is usually because the filesystem \u0026#34;vboxsf\u0026#34; is not available. This filesystem is made available via the VirtualBox Guest Additions and kernel module. Please verify that these guest additions are properly installed in the guest. This is not a bug in Vagrant and is usually caused by a faulty Vagrant box. For context, the command attempted was: mount -t vboxsf -o uid=1000,gid=1000 kfslab /kfslab The error output from the command was: mount: unknown filesystem type \u0026#39;vboxsf\u0026#39; 需要安装 guest\nvm.rb:649:in `initialize\u0026rsquo;: no implicit conversion of nil into String (TypeError) 我在 Vagrantfile 里使用了环境变量 $KFS_HOME , 如果未初始化（不如新开一个终端窗口），执行 vagrant up 就会出现下面错误：\n➜ vos vagrant up Bringing machine \u0026#39;k8s-master-1\u0026#39; up with \u0026#39;virtualbox\u0026#39; provider... Bringing machine \u0026#39;k8s-node-1\u0026#39; up with \u0026#39;virtualbox\u0026#39; provider... Bringing machine \u0026#39;k8s-node-2\u0026#39; up with \u0026#39;virtualbox\u0026#39; provider... /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/kernel_v2/config/vm.rb:649:in `initialize\u0026#39;: no implicit conversion of nil into String (TypeError) from /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/kernel_v2/config/vm.rb:649:in `new\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/kernel_v2/config/vm.rb:649:in `block in validate\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/kernel_v2/config/vm.rb:644:in `each\u0026#39; from /opt/vagrant/embedded/gems/2.2.4/gems/vagrant-2.2.4/plugins/kernel_v2/config/vm.rb:644:in `validate\u0026#39; 解决方案：\nsource $KFS_HOME/setting"
},
{
	"uri": "http://example.org/helm/",
	"title": "Helm",
	"tags": ["kubernetes", "helm"],
	"description": "",
	"content": " The package manager for Kubernetes\n https://helm.sh/  安装 brew install kubernetes-helm 初始化 初始化并部署 Tiller\nhelm init --service-account tiller --history-max 200 --tiller-image=omio/gcr.io.kubernetes-helm.tiller:v2.13.1 注意\n Role-based Access Control  "
},
{
	"uri": "http://example.org/kfs/v1.14.1/pki/",
	"title": "配置 PKI",
	"tags": ["v1.14.1", "Kubernetes", "拓扑图"],
	"description": "",
	"content": " 在 Kubernetes 集群中，各个组件间通过 TLS 进行通信。组件的证书可以代表其唯一性（ Common Name ）。搭建 PKI 可以自由签发证书给需要的组件。\n初始化 CA 初始化 Certificate Authority ，创建 pki 目录：\nmkdir -pv ${KFS_CONFIG} cd ${KFS_CONFIG} 创建 ca-config.json ：\n{ \u0026#34;signing\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;expiry\u0026#34;: \u0026#34;8760h\u0026#34; }, \u0026#34;profiles\u0026#34;: { \u0026#34;kubernetes\u0026#34;: { \u0026#34;usages\u0026#34;: [\u0026#34;signing\u0026#34;, \u0026#34;key encipherment\u0026#34;, \u0026#34;server auth\u0026#34;, \u0026#34;client auth\u0026#34;], \u0026#34;expiry\u0026#34;: \u0026#34;8760h\u0026#34; } } } } 创建 ca-csr.json ：\n{ \u0026#34;CN\u0026#34;: \u0026#34;KFS\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;KFS\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;CA\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建 ca.pem 和 ca-key.pem ：\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca 提示 cfssl 可以查看指定证书详情\ncfssl certinfo -cert ca.pem 一些客户端证书 admin 创建 admin-csr.json :\n{ \u0026#34;CN\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:masters\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建 admin.pem 和 admin-key.pem ：\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  admin-csr.json | cfssljson -bare admin kubelet 说明 我们有2个节点，匹配创建脚本如下\nNODES=$(cat \u0026lt;\u0026lt;-END k8s-node-1 192.168.100.31 k8s-node-2 192.168.100.32 END ) function genpem() { name=$1 ip=$2 echo $ip $hostname cat \u0026gt; ${name}-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;system:node:${name}\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:nodes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } EOF cfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -hostname=${name},${ip} \\  -profile=kubernetes \\  ${name}-csr.json | cfssljson -bare ${name} } while read -r line; do a=($(echo \u0026#34;$line\u0026#34; | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39;)) name=\u0026#34;${a[0]}\u0026#34; ip=\u0026#34;${a[1]}\u0026#34; genpem $ip $name done \u0026lt;\u0026lt;\u0026lt; \u0026#34;$NODES\u0026#34; 生成\n➜ pki ls -al k8s-node-*.pem -rw------- 1 gwind staff 1679 May 8 18:03 k8s-node-1-key.pem -rw-r--r-- 1 gwind staff 1468 May 8 18:03 k8s-node-1.pem -rw------- 1 gwind staff 1679 May 8 18:03 k8s-node-2-key.pem -rw-r--r-- 1 gwind staff 1468 May 8 18:03 k8s-node-2.pem kube-controller-manager 创建 kube-controller-manager-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;system:kube-controller-manager\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:kube-controller-manager\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建 kube-controller-manager.pem 和 kube-controller-manager-key.pem\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager kube-proxy 创建 kube-proxy-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;system:kube-proxy\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:node-proxier\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建 kube-proxy.pem 和 kube-proxy-key.pem\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  kube-proxy-csr.json | cfssljson -bare kube-proxy kube-scheduler 创建 kube-scheduler-csr.json\n{ \u0026#34;CN\u0026#34;: \u0026#34;system:kube-scheduler\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:kube-scheduler\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建证书:\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  kube-scheduler-csr.json | cfssljson -bare kube-scheduler kube-apiserver 创建 kubernetes-csr.json :\n{ \u0026#34;CN\u0026#34;: \u0026#34;kubernetes\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;Kubernetes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建证书：\nKUBERNETES_PUBLIC_ADDRESS=\u0026#34;192.168.100.11\u0026#34; cfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -hostname=10.32.0.1,${KUBERNETES_PUBLIC_ADDRESS},127.0.0.1,kubernetes.default \\  -profile=kubernetes \\  kubernetes-csr.json | cfssljson -bare kubernetes Service Account 创建 service-account-csr.json :\n{ \u0026#34;CN\u0026#34;: \u0026#34;service-accounts\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;Kubernetes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } 创建证书：\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  service-account-csr.json | cfssljson -bare service-account 分发到各个服务器 k8s-master-1 需要：\n ca.pem ca-key.pem kubernetes.pem kubernetes-key.pem kube-controller-manager.pem kube-controller-manager-key.pem kube-scheduler.pem kube-scheduler-key.pem service-account.pem service-account-key.pem  k8s-node-1 , k8s-node-2 需要：\n ca.pem k8s-node-{N}.pem k8s-node-{N}-key.pem kube-proxy.pem kube-proxy-key.pem  注意\n 我们挂载了 Host 的 $KFS_HOME 到每一个 Guest 虚拟机的 /kfslab 目录。因此只需要到时候系统内 copy 即可。生产环境请在部署完成后，确保 node 上只包含自己需要的证书，确保安全。  "
},
{
	"uri": "http://example.org/kfs/v1.14.1/kubeconfig/",
	"title": "配置 kubeconfig",
	"tags": ["v1.14.1", "Kubernetes", "kubeconfig"],
	"description": "",
	"content": " 创建配置文件 mkdir -pv ${KFS_CONFIG} cd ${KFS_CONFIG} # 确保 KUBERNETES_PUBLIC_ADDRESS 值是正确的 echo $KUBERNETES_PUBLIC_ADDRESS kubelet 为每个节点的 kubelet 创建独立的配置文件：\nfor instance in k8s-node-1 k8s-node-2; do kubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\  --kubeconfig=${instance}.kubeconfig kubectl config set-credentials system:node:${instance} \\  --client-certificate=${instance}.pem \\  --client-key=${instance}-key.pem \\  --embed-certs=true \\  --kubeconfig=${instance}.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=system:node:${instance} \\  --kubeconfig=${instance}.kubeconfig kubectl config use-context default --kubeconfig=${instance}.kubeconfig done 生成 2 个节点的 kubeconfig 配置文件：\n$ ls -al k8s-node-*.kubeconfig -rw------- 1 gwind staff 6288 May 9 10:03 k8s-node-1.kubeconfig -rw------- 1 gwind staff 6288 May 9 10:03 k8s-node-2.kubeconfig kube-proxy 所有节点的 kube-proxy 使用相同的配置文件，创建 kube-proxy.kubeconfig :\nkubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\  --kubeconfig=kube-proxy.kubeconfig kubectl config set-credentials system:kube-proxy \\  --client-certificate=kube-proxy.pem \\  --client-key=kube-proxy-key.pem \\  --embed-certs=true \\  --kubeconfig=kube-proxy.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=system:kube-proxy \\  --kubeconfig=kube-proxy.kubeconfig kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig kube-controller-manager 创建 kube-controller-manager.kubeconfig\nkubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://127.0.0.1:6443 \\  --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-credentials system:kube-controller-manager \\  --client-certificate=kube-controller-manager.pem \\  --client-key=kube-controller-manager-key.pem \\  --embed-certs=true \\  --kubeconfig=kube-controller-manager.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=system:kube-controller-manager \\  --kubeconfig=kube-controller-manager.kubeconfig kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig kube-scheduler 创建 kube-scheduler.kubeconfig ：\nkubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://127.0.0.1:6443 \\  --kubeconfig=kube-scheduler.kubeconfig kubectl config set-credentials system:kube-scheduler \\  --client-certificate=kube-scheduler.pem \\  --client-key=kube-scheduler-key.pem \\  --embed-certs=true \\  --kubeconfig=kube-scheduler.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=system:kube-scheduler \\  --kubeconfig=kube-scheduler.kubeconfig kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig admin 创建 admin.kubeconfig ：\nkubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://127.0.0.1:6443 \\  --kubeconfig=admin.kubeconfig kubectl config set-credentials admin \\  --client-certificate=admin.pem \\  --client-key=admin-key.pem \\  --embed-certs=true \\  --kubeconfig=admin.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=admin \\  --kubeconfig=admin.kubeconfig kubectl config use-context default --kubeconfig=admin.kubeconfig admin-mbp 说明 创建这个 admin 配置文件，是为了从 mbp （集群外部）访问集群。\n创建 admin-mbp.kubeconfig ：\nkubectl config set-cluster kfs \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://192.168.100.11:6443 \\  --kubeconfig=admin-mbp.kubeconfig kubectl config set-credentials admin \\  --client-certificate=admin.pem \\  --client-key=admin-key.pem \\  --embed-certs=true \\  --kubeconfig=admin-mbp.kubeconfig kubectl config set-context default \\  --cluster=kfs \\  --user=admin \\  --kubeconfig=admin-mbp.kubeconfig kubectl config use-context default --kubeconfig=admin-mbp.kubeconfig 分发配置文件 k8s-master-1 需要：\n admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig  k8s-node-1 , k8s-node-2 需要：\n node-{N}.kubeconfig kube-proxy.kubeconfig  Other Data Encryption Config and Key 创建 encryption-config.yaml :\nENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64) cat \u0026gt; encryption-config.yaml \u0026lt;\u0026lt;EOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {} EOF"
},
{
	"uri": "http://example.org/kfs/v1.14.1/install-master/",
	"title": "搭建 K8S Master",
	"tags": ["v1.14.1", "Kubernetes", "拓扑图"],
	"description": "",
	"content": " 准备 从 mbp 登录 k8s-master-1 虚拟机：\nssh -v k8s-master-1 检查 /kfslab 目录是否挂载正确（也可以使用 rsync 同步）：\n# ls /kfslab/ config install setting v1.14.1 vos 设置 K8S_MASTER_ROOT 环境变量和创建目录：\nexport K8S_MASTER_ROOT=\u0026#34;/root/lab\u0026#34; mkdir -pv $K8S_MASTER_ROOT 设置基本环境变量：\nunalias cp cd $K8S_MASTER_ROOT cat \u0026gt; k8s-master-setting \u0026lt;\u0026lt;EOF KFS_HOME=/kfslab KFS_CONFIG=\u0026#34;\\${KFS_HOME}/config\u0026#34; KFS_INSTALL=\u0026#34;\\${KFS_HOME}/install\u0026#34; K8S_VERSION=v1.14.1 K8S_MASTER_ROOT=\u0026#34;/root/lab\u0026#34; EOF source k8s-master-setting 注意 每一次中断部署，再次返回，请先是环境变量生效：\nexport K8S_MASTER_ROOT=\u0026#34;/root/lab\u0026#34; cd $K8S_MASTER_ROOT source k8s-master-setting Master 部署 etcd 准备目录：\n# 确保目录存在 mkdir -pv /etc/etcd # 拷贝证书 cd $KFS_CONFIG cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd # 拷贝二进制程序 cd $KFS_INSTALL/master/bin/ cp etcd etcdctl /usr/local/bin/INTERNAL_IP=\u0026#34;192.168.100.11\u0026#34; ETCD_NAME=$(hostname -s) cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/etcd.service [Unit] Description=etcd Documentation=https://github.com/coreos [Service] Type=notify ExecStart=/usr/local/bin/etcd \\\\ --name ${ETCD_NAME} \\\\ --cert-file=/etc/etcd/kubernetes.pem \\\\ --key-file=/etc/etcd/kubernetes-key.pem \\\\ --peer-cert-file=/etc/etcd/kubernetes.pem \\\\ --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\ --trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\ --peer-client-cert-auth \\\\ --client-cert-auth \\\\ --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-peer-urls https://${INTERNAL_IP}:2380 \\\\ --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\\\ --advertise-client-urls https://${INTERNAL_IP}:2379 \\\\ --initial-cluster-token etcd-cluster-0 \\\\ --initial-cluster k8s-master-1=https://${INTERNAL_IP}:2380 \\\\ --initial-cluster-state new \\\\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF 启动服务\nsystemctl daemon-reload systemctl enable etcd systemctl start etcd 测试 etcd 服务是否 OK\nETCDCTL_API=3 etcdctl member list \\  --endpoints=https://127.0.0.1:2379 \\  --cacert=/etc/etcd/ca.pem \\  --cert=/etc/etcd/kubernetes.pem \\  --key=/etc/etcd/kubernetes-key.pem kubernetes master components 本部分我们启动 kubernetes master 相关组件。\n准备：\n# 复制 kubernetes master 需要的程序 cd $KFS_INSTALL/master/bin/ cp kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ # 复制 kubernetes master 需要的配置文件 mkdir -pv /etc/kubernetes/ cd $KFS_CONFIG cp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem service-account-key.pem service-account.pem encryption-config.yaml /etc/kubernetes/ cp kube-controller-manager.kubeconfig /etc/kubernetes/ cp kube-scheduler.kubeconfig /etc/kubernetes/ kube-apiserver 创建 /etc/systemd/system/kube-apiserver.service :\nINTERNAL_IP=\u0026#34;192.168.100.11\u0026#34; ETCD_SERVERS=\u0026#34;https://192.168.100.11:2379\u0026#34; cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=1 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/etc/kubernetes/ca.pem \\\\ --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/etc/kubernetes/ca.pem \\\\ --etcd-certfile=/etc/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/etc/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=${ETCD_SERVERS} \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/etc/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/etc/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/etc/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/etc/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/etc/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/etc/kubernetes/kubernetes-key.pem \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF kube-controller-manager 创建 /etc/systemd/system/kube-controller-manager.service :\nPOD_CIDR=\u0026#34;172.16.0.0/16\u0026#34; cat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=${POD_CIDR} \\\\ --allocate-node-cidrs \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/etc/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/etc/kubernetes/ca-key.pem \\\\ --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/etc/kubernetes/ca.pem \\\\ --service-account-private-key-file=/etc/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF kube-schedular 创建 kube-scheduler.yaml :\nmkdir -p /etc/kubernetes/config/ cat \u0026lt;\u0026lt;EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml apiVersion: kubescheduler.config.k8s.io/v1alpha1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \u0026#34;/etc/kubernetes/kube-scheduler.kubeconfig\u0026#34; leaderElection: leaderElect: true EOF 创建 kube-scheduler.service\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Start Kubernetes Master Components 启动 Kubernetes Master 组件：\nsystemctl daemon-reload systemctl enable kube-apiserver kube-controller-manager kube-scheduler systemctl start kube-apiserver kube-controller-manager kube-scheduler 查看状态有多种方法：\n# systemd 服务状态 systemctl status kube-apiserver kube-controller-manager kube-scheduler # 查看实时日志 journalctl -u kube-apiserver -f journalctl -u kube-controller-manager -f journalctl -u kube-scheduler -f 部署 nginx 以支持默认 80 端口的 /healthz\nyum install -y nginx cat \u0026gt; /etc/nginx/conf.d/kubernetes.default.svc.cluster.local.conf \u0026lt;\u0026lt;EOF server { listen 80; server_name kubernetes.default.svc.cluster.local; location /healthz { proxy_pass https://127.0.0.1:6443/healthz; proxy_ssl_trusted_certificate /var/lib/kubernetes/ca.pem; } } EOF systemctl start nginx 测试 http://127.0.0.1/healthz 服务是否正确：\n[root@k8s-master-1 lab]# curl -H \u0026#34;Host: kubernetes.default.svc.cluster.local\u0026#34; -i http://127.0.0.1/healthz HTTP/1.1 200 OK Server: nginx/1.12.2 Date: Fri, 10 May 2019 03:28:15 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 2 Connection: keep-alive X-Content-Type-Options: nosniff 测试 内部访问集群 测试内部访问集群，我们在 k8s-master-1 执行测试操作：\n准备：\ncd $KFS_CONFIG cp admin.kubeconfig $K8S_MASTER_ROOT cd $K8S_MASTER_ROOT 查看 Kubernetes 集群组件状态 [root@k8s-master-1 lab]# kubectl get componentstatuses --kubeconfig admin.kubeconfig NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 外部访问集群 测试外部访问集群，我们在 mbp 执行测试操作。\n设置 kubectl 默认配置文件路径：\nexport KUBECONFIG=\u0026#34;${KFS_CONFIG}/admin-mbp.kubeconfig\u0026#34; 测试 healthz ：\n$ curl -H \u0026#34;Host: kubernetes.default.svc.cluster.local\u0026#34; -i http://192.168.100.11/healthz HTTP/1.1 200 OK Server: nginx/1.12.2 Date: Fri, 10 May 2019 03:31:23 GMT Content-Type: text/plain; charset=utf-8 Content-Length: 2 Connection: keep-alive X-Content-Type-Options: nosniff ok 查看集群组件 $ kubectl get componentstatuses --kubeconfig admin-mbp.kubeconfig NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;} 访问 kube-apiserver 接口 在外部测试（需要 ca.pem 和 k8s-master-1 的 IP）：\nKUBERNETES_PUBLIC_ADDRESS=\u0026#34;192.168.100.11\u0026#34; curl --cacert ca.pem \u0026#34;https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version\u0026#34; 返回结果示例：\n{ \u0026#34;major\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;minor\u0026#34;: \u0026#34;14\u0026#34;, \u0026#34;gitVersion\u0026#34;: \u0026#34;v1.14.1\u0026#34;, \u0026#34;gitCommit\u0026#34;: \u0026#34;b7394102d6ef778017f2ca4046abbaa23b88c290\u0026#34;, \u0026#34;gitTreeState\u0026#34;: \u0026#34;clean\u0026#34;, \u0026#34;buildDate\u0026#34;: \u0026#34;2019-04-08T17:02:58Z\u0026#34;, \u0026#34;goVersion\u0026#34;: \u0026#34;go1.12.1\u0026#34;, \u0026#34;compiler\u0026#34;: \u0026#34;gc\u0026#34;, \u0026#34;platform\u0026#34;: \u0026#34;linux/amd64\u0026#34; } 其他配置 RBAC for Kubelet Authorization kube-apiserver 访问 kubelet 需要配置 RBAC 权限。\n进入 k8s-master-1 服务器， 创建一个名为 system:kube-apiserver-to-kubelet 的 ClusterRole\ncat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubelet rules: - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \u0026#34;*\u0026#34; EOF kube-apiserver 通过 --kubelet-client-certificate 选项指定的证书，作为 kubernetes 用户访问 kubelet ，绑定上面创建的 system:kube-apiserver-to-kubelet ClusterRole 到 kubernetes 用户:\ncat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:kube-apiserver namespace: \u0026#34;\u0026#34; roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubelet subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kubernetes EOF"
},
{
	"uri": "http://example.org/kfs/v1.14.1/install-node/",
	"title": "搭建 K8S Node",
	"tags": ["v1.14.1", "Kubernetes", "拓扑图"],
	"description": "",
	"content": " 准备 下载软件包 说明 在 mbp 下载需要的软件包到 $KFS_INSTALL/node 目录。\n runc CNI plugins cri-tools containerd kube-proxy kubelet  cd $KFS_INSTALL/node wget -q --show-progress --https-only --timestamping \\  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc8/runc.amd64 \\  https://github.com/containernetworking/plugins/releases/download/v0.7.5/cni-plugins-amd64-v0.7.5.tgz \\  https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.14.0/crictl-v1.14.0-linux-amd64.tar.gz \\  https://github.com/containerd/containerd/releases/download/v1.2.6/containerd-1.2.6.linux-amd64.tar.gz 说明\n 上面 wget 用法在 MacOS 下才可以  网络 Kubernetes 通常使用一个扁平的网络空间（所有 pod 之间网络互动，无论在不在同一个节点），最简单的是 flannel 。\n如果您想要深入理解网络配置，请参考 Flannel From Scratch 提前在所有节点手动部署好 flannel 。 只需要 kubernetes 集群的 etcd 服务搭建好了之后，就可以部署各个节点的 flanneld 服务。\nNode 部署 说明\n 本章流程如无特殊说明，需要在 k8s-node-1 和 k8s-node-2 上都进行。 下面示例中，遇到特定主机名等信息，以 k8s-node-1 为例。请修改为正在部署节点的具体信息。  设置环境变量 设置 K8S_NODE_ROOT 环境变量和创建目录：\nexport K8S_NODE_ROOT=\u0026#34;/root/lab\u0026#34; mkdir -pv $K8S_NODE_ROOT 设置基本环境变量：\nunalias cp cd $K8S_NODE_ROOT cat \u0026gt; k8s-node-setting \u0026lt;\u0026lt;EOF KFS_HOME=/kfslab KFS_CONFIG=\u0026#34;\\${KFS_HOME}/config\u0026#34; KFS_INSTALL=\u0026#34;\\${KFS_HOME}/install\u0026#34; K8S_VERSION=v1.14.1 K8S_NODE_ROOT=\u0026#34;/root/lab\u0026#34; EOF source k8s-node-setting 注意 每一次中断部署，再次返回，请先是环境变量生效：\nexport K8S_NODE_ROOT=\u0026#34;/root/lab\u0026#34; cd $K8S_NODE_ROOT source k8s-node-setting 系统准备 操作系统软件包依赖：\n socat (执行 kubectl port-forward 需要）  安装依赖软件包：\nyum install -y socat conntrack ipset 创建目录\nmkdir -p \\  /etc/cni/net.d \\  /opt/cni/bin 注意\n /etc/cni/net.d 和 /opt/cni/bin 默认命名是这样，不要和已有环境混淆，也不要随便修改命名，除非您非常清楚为什么这样做。  复制二进制程序：\ncd $KFS_INSTALL/node/bin chmod a+x kube-proxy kubelet runc.amd64 cp runc.amd64 /usr/local/bin/runc cp kube-proxy kubelet /usr/local/bin/ cd $KFS_INSTALL/node tar -xvf crictl-v1.14.0-linux-amd64.tar.gz -C /usr/local/bin/ tar -xvf cni-plugins-amd64-v0.7.5.tgz -C /opt/cni/bin/ mkdir -pv /tmp/cni-plugins-amd64-v0.7.5 tar -xvf containerd-1.2.6.linux-amd64.tar.gz -C /tmp/cni-plugins-amd64-v0.7.5 cp /tmp/cni-plugins-amd64-v0.7.5/bin/* /usr/local/bin/ CNI 重要 :\n 这里认为您已经部署好了 flanneld , 配置 CNI 以便 containerd 能够获得正确的网络设置（参考 Flannel From Scratch） POD_CIDR 是在 Master 服务器，由 kube-controller-manager 程序启动时通过 --cluster-cidr=172.16.0.0/16 指定 POD_CIDR 如果某个节点上无需启动 kubelet , containerd 等 kubernetes node 角色，则无需配置这里的 CNI 。如本实验的 k8s-master-1 就无需配置。  # 手动启动 flanneld 服务，下面的文件一定存在，且已经获得了正确的网络端配置信息 source /run/flannel/subnet.env # 创建 bridge cat \u0026lt;\u0026lt;EOF | sudo tee /etc/cni/net.d/10-bridge.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cnio0\u0026#34;, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;ranges\u0026#34;: [ [{\u0026#34;subnet\u0026#34;: \u0026#34;${FLANNEL_SUBNET}\u0026#34;}] ], \u0026#34;routes\u0026#34;: [{\u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;}] } } EOF # 创建 loopback cat \u0026lt;\u0026lt;EOF | sudo tee /etc/cni/net.d/99-loopback.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;loopback\u0026#34; } EOF 注意\n 由于我使用 flannel 配置网络，启动 flanneld 后，本机的 /run/flannel/subnet.env 会包含一个分配好的 podCIDR 信息。我们需要使用这里的子网（即 FLANNEL_SUBNET 变量值）来配置 cni 。  containerd mkdir -p /etc/containerd/ cat \u0026lt;\u0026lt; EOF | sudo tee /etc/containerd/config.toml subreaper = true oom_score = -999 [debug] level = \u0026#34;debug\u0026#34; [metrics] address = \u0026#34;127.0.0.1:1338\u0026#34; [plugins.linux] runtime = \u0026#34;runc\u0026#34; shim_debug = true [plugins] [plugins.cri.containerd] snapshotter = \u0026#34;overlayfs\u0026#34; [plugins.cri.containerd.default_runtime] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34; runtime_engine = \u0026#34;/usr/local/bin/runc\u0026#34; runtime_root = \u0026#34;\u0026#34; [plugins.cri.containerd.untrusted_workload_runtime] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34; runtime_engine = \u0026#34;/usr/local/bin/runsc\u0026#34; runtime_root = \u0026#34;/run/containerd/runsc\u0026#34; [plugins.cri.containerd.gvisor] runtime_type = \u0026#34;io.containerd.runtime.v1.linux\u0026#34; runtime_engine = \u0026#34;/usr/local/bin/runsc\u0026#34; runtime_root = \u0026#34;/run/containerd/runsc\u0026#34; [plugins.cri] # sandbox_image is the image used by sandbox container. sandbox_image = \u0026#34;ibmcom/pause:3.1\u0026#34; EOF 说明\n containerd 详细的配置参考 https://github.com/containerd/cri/blob/master/docs/config.md 默认的 k8s.gcr.io/pause:3.1 资源在国内无法访问。国内网络需要配置 sandbox_image 为一个可以访问的路径；必须在这里配置，在 kubelet 使用 --pod-infra-container-image 配置无效。 可以使用 containerd config default 查看默认的配置（可以基于此修改） 暂时没有使用 /usr/local/bin/runsc 运行安全容器，可以先配置。  创建 containerd.service :\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target EOF kubelet HOSTNAME=\u0026#34;k8s-node-1\u0026#34; mkdir -p /etc/kubelet /etc/kubernetes/ cd $KFS_CONFIG cp ${HOSTNAME}-key.pem ${HOSTNAME}.pem /etc/kubelet/ cp ${HOSTNAME}.kubeconfig /etc/kubelet/kubeconfig cp ca.pem /etc/kubernetes/ 创建 kubelet-config.yaml:\nPOD_CIDR=\u0026#34;172.16.0.0/16\u0026#34; cat \u0026lt;\u0026lt;EOF | sudo tee /etc/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \u0026#34;/etc/kubernetes/ca.pem\u0026#34; authorization: mode: Webhook clusterDomain: \u0026#34;cluster.local\u0026#34; clusterDNS: - \u0026#34;10.32.0.10\u0026#34; podCIDR: \u0026#34;${POD_CIDR}\u0026#34; resolvConf: \u0026#34;/etc/resolv.conf\u0026#34; runtimeRequestTimeout: \u0026#34;15m\u0026#34; tlsCertFile: \u0026#34;/etc/kubelet/${HOSTNAME}.pem\u0026#34; tlsPrivateKeyFile: \u0026#34;/etc/kubelet/${HOSTNAME}-key.pem\u0026#34; EOF 注意\n resolvConf 在使用 systemd-resolved 的系统中需要避免 CoreDNS 的循环解析。 在 Ubuntu 下是 /run/systemd/resolve/resolv.conf 。本实验是 centos/7 , 暂时使用 /etc/resolv.conf 如果是 k8s-master-1 节点启动 kubelet ，我们可以使用 --hostname-override=node-0 指定一个自定义的主机名 node-0 。需要设置节点的 node-role.kubernetes.io/master 标签：  kubectl patch node 节点名 -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;taints\u0026#34;:[{\u0026#34;effect\u0026#34;:\u0026#34;NoSchedule\u0026#34;,\u0026#34;key\u0026#34;:\u0026#34;node-role.kubernetes.io/master\u0026#34;}]}}\u0026#39; 创建 kubelet.service ：\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --config=/etc/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/etc/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF kube-proxy mkdir -p /etc/kube-proxy cd $KFS_CONFIG cp kube-proxy.kubeconfig /etc/kube-proxy/kubeconfig 创建 kube-proxy-config.yaml:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/kube-proxy/kube-proxy-config.yaml kind: KubeProxyConfiguration apiVersion: kubeproxy.config.k8s.io/v1alpha1 clientConnection: kubeconfig: \u0026#34;/etc/kube-proxy/kubeconfig\u0026#34; mode: \u0026#34;iptables\u0026#34; clusterCIDR: \u0026#34;10.200.0.0/16\u0026#34; EOF 创建 kube-proxy.service:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --config=/etc/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF 启动 node 服务 systemctl daemon-reload systemctl enable containerd kubelet kube-proxy systemctl start containerd kubelet kube-proxy 测试 查看集群，是否能否发现新加入的节点：\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-node-1 Ready \u0026lt;none\u0026gt; 11m v1.14.1 10.0.2.15 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.12.1.el7.x86_64 containerd://1.2.6 k8s-node-2 Ready \u0026lt;none\u0026gt; 6s v1.14.1 10.0.2.15 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-957.12.1.el7.x86_64 containerd://1.2.6"
},
{
	"uri": "http://example.org/kfs/v1.14.1/flannel-from-scratch/",
	"title": "Flannel From Scratch",
	"tags": ["v1.14.1", "kfs", "network", "flannel"],
	"description": "",
	"content": " 简介 本章介绍手动部署 flanneld 的方法。\n说明：\n flanneld 在每个 host 上部署，通常我们会在集群的所有节点上部署。 所有 flanneld 之间通过 etcd 共享分布式配置，比如每个主机的 \u0026ldquo;子网段\u0026rdquo; 是多少。 containerd 通过 cni 机制从本机的 flanneld 管理的 “子网段” 中为即将运行的 container 拿到网络配置。 所有使用了 flanneld 管理的网络内的 container 即可属于同一个网络。  准备 下载 flannel 请在 mbp 上执行：\ncd $KFS_INSTALL/node wget -q --show-progress --https-only --timestamping \\  https://github.com/coreos/flannel/releases/download/v0.11.0/flanneld-amd64 \\  https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz 创建 flanneld 证书 与 etcd 通信需要。\n创建 flanneld-csr.json ，请在 mbp 上执行：\ncd $KFS_CONFIG cat \u0026gt; flanneld-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;system:flanneld\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:flanneld\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes From Scratch\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34; } ] } EOF 创建证书 ：\ncfssl gencert \\  -ca=ca.pem \\  -ca-key=ca-key.pem \\  -config=ca-config.json \\  -profile=kubernetes \\  flanneld-csr.json | cfssljson -bare flanneld 部署 请在任何需要 flanneld 的节点上执行部署操作。比如： k8s-master-1 , k8s-node-1, k8s-node-2\n准备证书 复制证书到需要的每个节点上 /etc/flanneld 目录，以 k8s-master-1 节点为例：\nmkdir -pv /etc/flanneld/ cd $KFS_HOME/config cp ca.pem flanneld.pem flanneld-key.pem /etc/flanneld/ 配置 etcd 在 etcd 中配置网络是一次性的，只需要集群搭建之处执行一次。\n在 k8s-master-1 机器执行：\nunset ETCDCTL_API etcdctl \\  --endpoints=https://192.168.100.11:2379 \\  --ca-file /etc/flanneld/ca.pem \\  --cert-file /etc/flanneld/flanneld.pem \\  --key-file /etc/flanneld/flanneld-key.pem \\  set /coreos.com/network/config \u0026#39;{ \u0026#34;Network\u0026#34;: \u0026#34;172.16.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: {\u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;}}\u0026#39; 注意 目前 flanneld 还不支持 etcd v3 协议，所以我们需要用旧的协议写入网络配置。\n启动 flanneld 拷贝 flanneld 程序：\ncd $KFS_INSTALL/node chmod a+x flanneld-amd64 cp flanneld-amd64 /usr/local/bin/flanneld 创建 flanneld.service :\nETCD_SERVERS=${1:-\u0026#34;https://192.168.100.11:2379\u0026#34;} FLANNEL_NET=${2:-\u0026#34;172.16.0.0/16\u0026#34;} CA_FILE=\u0026#34;/etc/flanneld/ca.pem\u0026#34; CERT_FILE=\u0026#34;/etc/flanneld/flanneld.pem\u0026#34; KEY_FILE=\u0026#34;/etc/flanneld/flanneld-key.pem\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/flanneld/flannel FLANNEL_ETCD=\u0026#34;--etcd-endpoints=${ETCD_SERVERS}\u0026#34; FLANNEL_ETCD_KEY=\u0026#34;--etcd-prefix=/coreos.com/network\u0026#34; FLANNEL_ETCD_CAFILE=\u0026#34;--etcd-cafile=${CA_FILE}\u0026#34; FLANNEL_ETCD_CERTFILE=\u0026#34;--etcd-certfile=${CERT_FILE}\u0026#34; FLANNEL_ETCD_KEYFILE=\u0026#34;--etcd-keyfile=${KEY_FILE}\u0026#34; EOF cat \u0026lt;\u0026lt;EOF \u0026gt;/usr/lib/systemd/system/flanneld.service [Unit] Description=Flanneld overlay address etcd agent After=network.target Before=containerd.service [Service] EnvironmentFile=-/etc/flanneld/flannel ExecStart=/usr/local/bin/flanneld --iface eth1 --ip-masq \\${FLANNEL_ETCD} \\${FLANNEL_ETCD_KEY} \\${FLANNEL_ETCD_CAFILE} \\${FLANNEL_ETCD_CERTFILE} \\${FLANNEL_ETCD_KEYFILE} Type=notify [Install] WantedBy=multi-user.target RequiredBy=containerd.service EOF 说明\n Vagrant 环境需要使用 --iface eth1 指定端口，否则所有 flanneld 获取的子网都一样。Running on Vagrant FLANNEL_NET 要和 kube-controller-manager 中的 --cluster-cidr 和 kubelet 配置文件中的 podCIDR 保持一致。   Vagrant has a tendency to give the default interface (one with the default route) a non-unique IP (often 10.0.2.15). This causes flannel to register multiple nodes with the same IP. To work around this issue, use \u0026ndash;iface option to specify the interface that has a unique IP. If you\u0026rsquo;re running on CoreOS, use cloud-config to set coreos.flannel.interface to $public_ipv4.\n 重启服务：\nsystemctl daemon-reload systemctl start flanneld 检查 flanneld 检查服务状态：\n# 服务状态应该是 active (running) systemctl status flanneld # 下面文件应该已经获取了可用的网络端配置信息 cat /run/flannel/subnet.env 检查 IP 是否能 ping 通：\n 在任意一个节点查看 ip 信息（ ip addr show flannel.1 | grep 172 ），如得到 ip 172.16.55.0 到另外一个节点 ping 该 ip ： ping 172.16.55.0 如果能 ping 通，说明 flanneld 服务配置正确  FAQ 为何手动部署 flannel ?  KFS 的宗旨是从零部署，加深理解。 我们希望 master 和 node 节点都可以相互 ping 通，方便从 master 管理各个节点。手动部署 flannel 比较简单，只需在 master 也启动一个 flanneld 即可。  为何 flanneld 获取的子网会变化 超过一定时间，重启服务，子网可能会发生变化。参考 Leases and Reservations\n生产环境建议在 kubernetes 部署 flanneld。\n参考  Running flannel  "
},
{
	"uri": "http://example.org/kfs/v1.14.1/flannel-in-k8s/",
	"title": "Flannel in kubernetes",
	"tags": ["v1.14.1", "kfs", "network", "flannel"],
	"description": "",
	"content": " 部署 kube-flannel.yml 如果未部署 Flannel From Scratch ，可以直接在集群中部署 flannel 。\n# 下载最新的 kube-flannel.yml wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml # 部署到 kubernetes kubectl apply -f kube-flannel.yml 说明\n 如果使用 vagrant 实验环境，需要在 flanneld 启动时使用 --iface=eth1 指定端口。编辑 kube-flannel.yml 配置文件。  FAQ pod cidr not assigned E0503 09:13:14.160194 1 main.go:289] Error registering network: failed to acquire lease: node \u0026#34;k8s-node-2\u0026#34; pod cidr not assigned"
},
{
	"uri": "http://example.org/kfs/v1.14.1/coredns/",
	"title": "CoreDNS",
	"tags": ["v1.14.1", "kfs", "dns", "coredns"],
	"description": "",
	"content": " CoreDNS 可以提供基于域名的服务访问。从 Kubernetes 1.13 开始，CoreDNS 作为默认的 DNS（之前是 kube-dns ）。\n https://github.com/coredns/coredns  提示 ：在 mbp 执行操作\n部署 CoreDNS 进入 kubernetes/cluster/addons/dns/coredns 目录：\ncd $KFS_HOME/v1.14.1/kubernetes/cluster/addons/dns/coredns 创建 coredns.yaml\nexport DNS_SERVER_IP=\u0026#34;10.32.0.10\u0026#34; export DNS_DOMAIN=\u0026#34;cluster.local\u0026#34; sed \\  -e \u0026#39;s/__PILLAR__DNS__SERVER__/\u0026#39;$DNS_SERVER_IP\u0026#39;/g\u0026#39; \\  -e \u0026#39;s/__PILLAR__DNS__DOMAIN__/\u0026#39;$DNS_DOMAIN\u0026#39;/g\u0026#39; \\  -e \u0026#39;s/k8s.gcr.io/coredns/g\u0026#39; \\  coredns.yaml.base \u0026gt; coredns.yaml 应用 coredns.yaml :\nkubectl apply -f coredns.yaml 查看 kubernetes 信息：\n$ kubectl get all --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system pod/coredns-8854569d4-csrzr 1/1 Running 0 33s 172.16.55.3 k8s-node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR default service/kubernetes ClusterIP 10.32.0.1 \u0026lt;none\u0026gt; 443/TCP 10h \u0026lt;none\u0026gt; kube-system service/kube-dns ClusterIP 10.32.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 33s k8s-app=kube-dns NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR kube-system deployment.apps/coredns 1/1 1 1 33s coredns coredns/coredns:1.3.1 k8s-app=kube-dns NAMESPACE NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR kube-system replicaset.apps/coredns-8854569d4 1 1 1 33s coredns coredns/coredns:1.3.1 k8s-app=kube-dns,pod-template-hash=8854569d4 测试 DNS 服务是否 OK # 运行一个 busybox kubectl run busybox --image=busybox:1.28 --command -- sleep 3600 # 查看 pods kubectl get pods -l run=busybox # 获取 POD_NAME POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) # 通过 nslookup 查看 DNS 服务是否正确 kubectl exec -ti $POD_NAME -- nslookup kubernetes 返回结果如下：\nServer: 10.32.0.10 Address 1: 10.32.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.32.0.1 kubernetes.default.svc.cluster.local"
},
{
	"uri": "http://example.org/kubernetes/security/",
	"title": "Security",
	"tags": ["Kubernetes", "Security"],
	"description": "",
	"content": " 11 Ways (Not) to Get Hacked  "
},
{
	"uri": "http://example.org/note/",
	"title": "文章",
	"tags": [],
	"description": "",
	"content": "和 Kubernetes 生态相关的文章。\n"
},
{
	"uri": "http://example.org/kfs/v1.14.1/dashboard/",
	"title": "Kubernetes Dashboard",
	"tags": ["v1.14.1", "Kubernetes", "Addons"],
	"description": "",
	"content": " 部署 Dashboard 说明 在 mbp 执行操作\n在 https://github.com/kubernetes/dashboard/releases 下载 v1.10.1 配置文件：\nmkdir -p $KFS_HOME/addons/dashboard cd $KFS_HOME/addons/dashboard wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 修改 kubernetes-dashboard.yaml ：\n k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1 修改为 omio/k8s.gcr.io.kubernetes-dashboard-amd64:v1.10.1  部署：\nkubectl apply -f kubernetes-dashboard.yaml 启动 kubectl proxy：\nkubectl proxy 等待 dashboard 部署完成，访问 http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nadmin user token 参考 Creating sample user\n创建\n# 创建配置文件 cat \u0026gt; kubernetes-dashboard-admin.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system EOF # 应用配置 kubectl apply -f kubernetes-dashboard-admin.yaml # 获取 token kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \u0026#39;{print $1}\u0026#39;) 最后一条命令输出如下：\nName: admin-user-token-w92l4 Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin-user kubernetes.io/service-account.uid: de39da99-6d55-11e9-98ed-525400261060 Type: kubernetes.io/service-account-token Data ==== namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXc5Mmw0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJkZTM5ZGE5OS02ZDU1LTExZTktOThlZC01MjU0MDAyNjEwNjAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.REHia-v_F2d38s78sZjMuY5H9AmJZjPVSBYoKwofhYqC73XYjk3aEk5nOUdkvENUv-X7jqhVKbbqxH0fsDAOZxHkM3MAE1X8b3FbqYAaz9K2EEI_LgmMGcxAG1I4pcPKB9glg1N4hj4nZoW4CweRSUftXTd1cGtaUfVyT9cJBcM_py7DKDe8OPjSMVcoezyseNsZKDdNZIKsg6FcYoQ5zbqBFM6PcKmYfKzfFRVNxZcjzIx9yINTCwlr_YBKnmu4BZDhu0Ty29X9R4uaDwOKjz-dx8nyq6jc3bP-lZy_2TtTUVgUd6qwx55Nm1lDl0kj7W1hFTFWyQSoEOFbU3yyrw ca.crt: 1318 bytes 其中 token 即可以在登录 dashboard 时使用。\n"
},
{
	"uri": "http://example.org/kfs/v1.14.1/",
	"title": "v1.14.1",
	"tags": ["v1.14.1", "Kubernetes"],
	"description": "",
	"content": " Kubernetes From Scratch（从零部署 Kubernetes）手册版本 v1.14.1 ，对应的 Kubernetes 发行版为 v1.14.1 。\n说明  Kubernetes 的版本发展比较快，请采用特定的版本，并选择依赖软件包的合适对应版本进行实践。 由于众所周知的原因, 本文档提及的部分资源需要在海外服务器操作。  资源  https://github.com/kubernetes/kubernetes/releases  参考  Creating a Custom Cluster from Scratch v1.13 及以后暂时没有该文档 Kubernetes The Hard Way https://github.com/ooclab/docs/tree/master/k8s  "
},
{
	"uri": "http://example.org/about/",
	"title": "关于",
	"tags": ["kubernetes", "helm"],
	"description": "",
	"content": "KFS 是 Kubernetes From Scratch 的缩写，项目命名学习 LFS (Linux From Sscrath)，同时在实践风格方面也学习 LFS。\n"
},
{
	"uri": "http://example.org/kfs/v1.14.1/faq/",
	"title": "FAQ",
	"tags": ["v1.14.1", "Kubernetes", "FAQ"],
	"description": "",
	"content": " 常见问题 如何重置测试环境数据 ？ 关闭并删除数据 Master 节点执行：\nsystemctl stop kube-apiserver kube-controller-manager kube-scheduler etcd flanneld rm -f /run/flannel/subnet.env rm -rf /var/lib/etcd 所有 Node 节点执行：\nsystemctl stop containerd kube-proxy kubelet flanneld rm -f /run/flannel/subnet.env 启动服务 Master 节点执行 flanneld 的网络配置：\nsystemctl start etcd unset ETCDCTL_API etcdctl \\  --endpoints=https://192.168.100.11:2379 \\  --ca-file /srv/kubernetes/certs/ca.pem \\  --cert-file /srv/kubernetes/certs/flanneld.pem \\  --key-file /srv/kubernetes/certs/flanneld-key.pem \\  set /coreos.com/network/config \u0026#39;{ \u0026#34;Network\u0026#34;: \u0026#34;172.16.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: {\u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;}}\u0026#39; Master 服务启动：\nsystemctl start kube-apiserver kube-controller-manager kube-scheduler flanneld 所有 Node 节点执行：\nsystemctl start containerd kube-proxy kubelet flanneld etcd tls: failed to verify client\u0026rsquo;s certificate: x509: certificate has expired or is not yet valid 详细错误如下：\nembed: rejected connection from \u0026#34;192.168.50.11:37744\u0026#34; (error \u0026#34;tls: failed to verify client\u0026#39;s certificate: x509: certificate has expired or is not yet valid\u0026#34;, ServerName \u0026#34;\u0026#34;) 最后发现因为 etcd 服务器的时间错误（由于使用 vagrant 虚拟机测试，暂停实践时，就把 mbp 合上盖休眠，因此虚拟机时间出错）。\n# 安装 ntpdate yum install -y ntpdate # 同步时间 ntpdate 0.pool.ntp.org 重启 etcd 服务，重试即可。\n更好的方法是，安装 ntpd 服务：\nyum install -y ntp systemctl enable ntpd systemctl start ntpd cni \u0026ldquo;cnio0\u0026rdquo; already has an IP address different from kubelet 日志错误详情如下：\nfailed to set bridge addr: \\\u0026#34;cnio0\\\u0026#34; already has an IP address different from 172.16.0.1/16\u0026#34; 我这里遇到的情况是之前配置了错误了 podCIDR 网段，因此 cnio0 网络设备已经创建并配置 OK。所以我需要删除该网络设备：\nip link del cnio0 重启 worker 相关服务即可。\ninvalid CIDR address kubelet 和 containerd 都看到该错误信息，通常是 kubelet 调用 containerd ，错误是同一个。 我遇到的情况是 /etc/cni/net.d/ 下 subnet 没有配置（为空）。\nDNS dial tcp: lookup k8s-node-2 on 10.0.2.3:53: no such host 测试 coredns 服务是否正确过程中，我们创建 busybox 服务并测试：\nkubectl run busybox --image=busybox:1.28 --command -- sleep 3600 POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl exec -ti $POD_NAME -- nslookup kubernetes 出现如下错误：\nError from server: error dialing backend: dial tcp: lookup k8s-node-2 on 10.0.2.3:53: no such host 原因是 k8s-node-2 主机名称没有通过 10.0.2.3:53 域名服务器查找到。10.0.2.3:53 是 vagrant 环境默认的 DNS 服务器。 即所有主机需要能否知道所有的节点自定义主机名（如：k8s-master-1, k8s-node-1, k8s-node-2等）的对于 IP。\n方法一：手动维护 /etc/hosts 192.168.100.11 k8s-master-1 192.168.100.31 k8s-node-1 192.168.100.32 k8s-node-2 Other error: unable to upgrade connection: Forbidden 执行 exec 或 log 子命令：\nkubectl exec -ti $POD_NAME -- nslookup kubernetes 详细错误如下：\nerror: unable to upgrade connection: Forbidden (user=kubernetes, verb=create, resource=nodes, subresource=proxy) 是因为 kube-apiserver 没有操作 kubelet 相关资源的权限，请确保 system:kube-apiserver-to-kubelet 权限及相关绑定已经存在：\n# kubectl get clusterrole system:kube-apiserver-to-kubelet NAME AGE system:kube-apiserver-to-kubelet 6m3s # kubectl get clusterrolebinding system:kube-apiserver NAME AGE system:kube-apiserver 4m50s SchemaError(io.k8s.api.autoscaling.v2beta2.ResourceMetricStatus) 我的实践中，使用了 mac 上 docker 服务启动安装的 kubectl 命令，版本较低，校验 kubernetes-dashboard.yaml v1.10.1 版本出错，详情如下：\n➜ kfslab kubectl apply -f kubernetes-dashboard.yaml error: SchemaError(io.k8s.api.autoscaling.v2beta2.ResourceMetricStatus): invalid object doesn\u0026#39;t have additional properties ➜ kfslab kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;10\u0026#34;, GitVersion:\u0026#34;v1.10.11\u0026#34;, GitCommit:\u0026#34;637c7e288581ee40ab4ca210618a89a555b6e7e9\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2018-11-26T14:38:32Z\u0026#34;, GoVersion:\u0026#34;go1.9.3\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;14\u0026#34;, GitVersion:\u0026#34;v1.14.1\u0026#34;, GitCommit:\u0026#34;b7394102d6ef778017f2ca4046abbaa23b88c290\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2019-04-08T17:02:58Z\u0026#34;, GoVersion:\u0026#34;go1.12.1\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 使用新版本的 kubectl 解决问题。\nunknown service runtime.v1alpha2.RuntimeService kubelet 启动错误信息：\nE0427 15:02:19.024109 8909 remote_runtime.go:85] Version from runtime service failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService E0427 15:02:19.024261 8909 kuberuntime_manager.go:196] Get runtime version failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService F0427 15:02:19.024289 8909 server.go:265] failed to run Kubelet: failed to create kubelet: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService 原因分析：一开始使用的系统安装的 docker\ncontainerd --version containerd github.com/containerd/containerd 1.2.5 bb71b10fd8f58240ca47fbb579b9d1028eea7c84 使用最新的 containerd 即可。\n"
},
{
	"uri": "http://example.org/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "KFS 是 Kubernetes From Scratch 的缩写，项目命名学习 LFS (Linux From Sscrath)，同时在实践风格方面也学习 LFS。\n KFS  v1.14.1  kubectl Helm Istio  "
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/design/",
	"title": "design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/env/",
	"title": "env",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/v1.14.1/",
	"title": "v1.14.1",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/vagrant/",
	"title": "vagrant",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/vbguest/",
	"title": "vbguest",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/note/vagrant-vbguest/",
	"title": "安装 vagrant-vbguest 插件",
	"tags": ["v1.14.1", "vagrant", "vbguest"],
	"description": "",
	"content": " 参考：\n VirtualBox Guest Additions https://github.com/dotless-de/vagrant-vbguest/  步骤 1. 下载 iso 在 https://download.virtualbox.org/virtualbox 找到对应的版本，下载并挂载 iso。示例：\nwget https://download.virtualbox.org/virtualbox/6.0.6/VBoxGuestAdditions_6.0.6.iso sudo cp VBoxGuestAdditions_6.0.6.iso /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso 2. 安装 vagrant plugin vagrant plugin install vagrant-vbguest 3. 配置 Vagrantfile 说明\n Mac 平台如果 /Applications/VirtualBox.app/Contents/MacOS/VBoxGuestAdditions.iso 存在，无需配置 iso_path 。\n Vagrantfile 中添加配置示例：\n# 共享目录（需要安装好 guest 驱动才可以注销掉下面几行） config.vm.synced_folder ENV[\u0026#34;KFS_HOME\u0026#34;], \u0026#34;/kfslab\u0026#34;, type: \u0026#34;virtualbox\u0026#34; if Vagrant.has_plugin?(\u0026#34;vagrant-vbguest\u0026#34;) config.vbguest.auto_update = false end 其他 强制重新安装\nvagrant vbguest --do install FAQ setup: command not found ==\u0026gt; k8s-node-1: Checking for guest additions in VM... The following SSH command responded with a non-zero exit status. Vagrant assumes that this means the command failed! setup Stdout from the command: Stderr from the command: bash: line 4: setup: command not found 参考\n vagrant-vbguest doesn\u0026rsquo;t work with vagrant-reload  解决方案，在配置文件增加\nif Vagrant.has_plugin?(\u0026#34;vagrant-vbguest\u0026#34;) config.vbguest.auto_update = false end"
},
{
	"uri": "http://example.org/tags/components/",
	"title": "Components",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/node/",
	"title": "node",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/istio/",
	"title": "istio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/flannel/",
	"title": "flannel",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/kfs/",
	"title": "kfs",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/network/",
	"title": "network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/prometheus/prometheus-operator/",
	"title": "Prometheus Operator",
	"tags": ["v1.14.1", "kubernetes", "addons", "prometheus"],
	"description": "",
	"content": "部署：\nwget https://raw.githubusercontent.com/coreos/prometheus-operator/master/bundle.yaml mv bundle.yaml prometheus-operator-bundle.yaml kubectl apply -f prometheus-operator-bundle.yaml"
},
{
	"uri": "http://example.org/tags/addons/",
	"title": "addons",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/prometheus/",
	"title": "prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/note/kubeconfig/",
	"title": "kubeconfig",
	"tags": [],
	"description": "",
	"content": " kubeconfig file 如果希望 kubectl 默认使用某个配置文件，可以：\n方法一：可以直接复制一个配置文件到指定位置： cp admin.kubeconfig ~/.kube/config 方法二：设置环境变量 export KUBECONFIG=/path/to/admin.kubeconfig"
},
{
	"uri": "http://example.org/note/crictl/",
	"title": "crictl",
	"tags": [],
	"description": "",
	"content": " crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a 配置 /etc/crictl.yaml :\ncat \u0026gt; /etc/crictl.yaml \u0026lt;\u0026lt;EOF runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: true EOF 现在执行 crictl 命令，无需指定 --runtime-endpoint：\ncrictl ps -a 加载 images 需要使用 containerd 安装安装包中的 ctr 命令\nctr -n=k8s.io images import /tmp/dashboard.tar 参考  CRICTL User Guide  "
},
{
	"uri": "http://example.org/kfs/v1.14.1/smoke_test/",
	"title": "Smoke Test",
	"tags": ["v1.14.1", "Kubernetes", "Testing"],
	"description": "",
	"content": " Data Encryption 创建一个加密数据：\nkubectl create secret generic kubernetes-the-hard-way \\ --from-literal=\u0026#34;mykey=mydata\u0026#34; 查看 etcd 中的加密数据\nETCDCTL_API=3 etcdctl get \\ --endpoints=https://192.168.50.10:2379 \\ --cacert=/etc/etcd/ca.pem \\ --cert=/etc/etcd/kubernetes.pem \\ --key=/etc/etcd/kubernetes-key.pem\\ /registry/secrets/default/kubernetes-the-hard-way | hexdump -C 结果如下：\n00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 6b 75 62 65 72 6e |s/default/kubern| 00000020 65 74 65 73 2d 74 68 65 2d 68 61 72 64 2d 77 61 |etes-the-hard-wa| 00000030 79 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 |y.k8s:enc:aescbc| 00000040 3a 76 31 3a 6b 65 79 31 3a 22 bb e9 96 78 5b d5 |:v1:key1:\u0026#34;...x[.| 00000050 81 47 04 2b 94 de 09 dc de 05 11 b7 fe 36 09 22 |.G.+.........6.\u0026#34;| 00000060 66 80 d1 c1 04 6c 81 e3 bc 3a 23 2c 05 db c8 92 |f....l...:#,....| 00000070 6e ff 93 79 2b 10 0e fa f8 54 58 52 f2 2a 7d d8 |n..y+....TXR.*}.| 00000080 c2 33 44 d3 9a 09 8b 07 d9 bc 2b 9e 95 a7 98 23 |.3D.......+....#| 00000090 0f db 96 f6 31 e9 c1 6c b2 13 00 b0 1d 5b 31 93 |....1..l.....[1.| 000000a0 df 9c b7 eb 57 92 a0 25 74 45 c1 26 6f 20 43 12 |....W..%tE.\u0026amp;o C.| 000000b0 9a 3e 7e 8d bd 53 de 15 15 97 04 59 16 90 f7 48 |.\u0026gt;~..S.....Y...H| 000000c0 ec 2c 52 75 91 20 9b 13 b0 99 c2 ac fd 0a 29 78 |.,Ru. ........)x| 000000d0 17 88 76 9c ae 23 8c 7b fe 75 14 7c cd cb de 7e |..v..#.{.u.|...~| 000000e0 e5 f0 e6 02 23 29 10 81 e0 0a |....#)....| 000000ea Deployments # 创建 nginx kubectl run nginx --image=nginx --generator=run-pod/v1 # 查看 pod kubectl get pods -l run=nginx # 或者 kubectl get pods -l run=nginx -o wide # 或者 kubectl get pod/nginx # 或者 kubectl get pod/nginx -o wide # 查看 pod 详情 kubectl describe pod/nginx 端口转发 在 k8s 集群中任一节点（部署了 flanneld , 有 kubectl , 且 kubectl 可以访问集群）， 测试 nginx 可访问性：\n# kubectl get pod/nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 4m45s 172.16.92.6 node-1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # curl http://172.16.92.6 在远程节点（有 kubectl 和 admin.kubeconfig 配置）执行：\n# POD_NAME=$(kubectl get pods -l run=nginx -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) # kubectl port-forward $POD_NAME 8080:80 Forwarding from 127.0.0.1:8080 -\u0026gt; 80 Forwarding from [::1]:8080 -\u0026gt; 80 在远程节点另外一个终端执行：\n# curl --head http://127.0.0.1:8080 HTTP/1.1 200 OK Server: nginx/1.15.12 Date: Thu, 02 May 2019 08:24:59 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT Connection: keep-alive ETag: \u0026#34;5cb5d3c3-264\u0026#34; Accept-Ranges: bytes 注意 1. pod 所在的节点必须安装有 socat 程序\nLogs kubectl logs $POD_NAME Exec kubectl exec -ti $POD_NAME -- nginx -v Services kubectl expose pod nginx --port 80 --type NodePort 获取端口:\nNODE_PORT=$(kubectl get svc nginx \\ --output=jsonpath=\u0026#39;{range .spec.ports[0]}{.nodePort}\u0026#39;) 使用任意节点的外部 IP 测试皆可访问：\ncurl -I http://192.168.50.11:${NODE_PORT} curl -I http://192.168.50.12:${NODE_PORT} 查看日志也可以发现：\nkubectl logs $POD_NAME --tail=2 172.16.92.2 - - [02/May/2019:08:32:57 +0000] \u0026#34;HEAD / HTTP/1.1\u0026#34; 200 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.54.0\u0026#34; \u0026#34;-\u0026#34; 172.16.32.0 - - [02/May/2019:08:33:02 +0000] \u0026#34;HEAD / HTTP/1.1\u0026#34; 200 0 \u0026#34;-\u0026#34; \u0026#34;curl/7.54.0\u0026#34; \u0026#34;-\u0026#34;"
},
{
	"uri": "http://example.org/tags/testing/",
	"title": "Testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/note/cni/",
	"title": "cni",
	"tags": [],
	"description": "",
	"content": " HOW A CONTAINER RUNTIME IS USING CNI  "
},
{
	"uri": "http://example.org/note/cfssl/",
	"title": "cfssl",
	"tags": [],
	"description": "",
	"content": "查看 ca.pem 证书信息：\ncfssl certinfo -cert ca.pem"
},
{
	"uri": "http://example.org/note/docker-image-gfw/",
	"title": "Docker image 在Qiang外怎么办？",
	"tags": [],
	"description": "",
	"content": "创建 trans_image.sh 脚本：\n#! /bin/bash  # https://hub.docker.com # 第一次运行需要执行 `docker login` 验证帐户 # 将指定的 image 转移到 hub.docker.com 上 # 使用 omio 组织名称 # https://hub.docker.com/u/omio/dashboard/ ORG=omio # gcr.io/google_containers/pause-amd64:3.0 -\u0026gt; $ORG/gcr.io.google_containers.pause-amd64:3.0 function trans() { ORIG_NAME=$1 NEW_NAME=`echo ${ORIG_NAME} | sed \u0026#39;s@/@.@g\u0026#39;` docker pull $ORIG_NAME docker tag $ORIG_NAME $ORG/$NEW_NAME docker push $ORG/$NEW_NAME } for var in \u0026#34;$@\u0026#34; do echo \u0026#34;==\u0026gt; $var\u0026#34; trans $var done 比如将 k8s.gcr.io/pause:3.1 转换为 omio/k8s.gcr.io.pause:3.1 ，执行：\nbash trans_image.sh k8s.gcr.io/pause:3.1 参考： - Docker image 在Qiang外怎么办？\n"
},
{
	"uri": "http://example.org/tags/kubectl/",
	"title": "kubectl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/coredns/",
	"title": "coredns",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/dns/",
	"title": "dns",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/kubeconfig/",
	"title": "kubeconfig",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/prepare/",
	"title": "prepare",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/%E6%8B%93%E6%89%91%E5%9B%BE/",
	"title": "拓扑图",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]